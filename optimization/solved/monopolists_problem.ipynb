{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monopolist's Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob runs a widget factory. He is the sole producer of his particular type of widget.\n",
    "The price, $p$, he recieves in the market of his widget is detemined the quantity, $q$, of widgets he puts on the market.\n",
    "\n",
    "He faces the quadratic demand curve $p(q) = 100 - q^{2}$.\n",
    "\n",
    "Which means, for example:\n",
    "\n",
    "* if he sells 1 widget he recieves \\$99 per widget: $p(1) = 100 - 1^{2} = 99$\n",
    "* if he sells 2 widgets he recieves \\$96 per widget: $p(1) = 100 - 2^{2} = 96$\n",
    "* if he sells 3 widgets he recieves \\$91 per widget: $p(1) = 100 - 3^{2} = 91$\n",
    "\n",
    "Bob, owns the factory and equipment in it; he cannot sell it. His cost per widget is 10. This gives $C(q)=10q$ as his total costs.\n",
    "\n",
    "Profit is given by $price\\ per\\ unit * units\\ sold - total\\ costs$. In our notation, Bob profit, $\\pi$ can be written as $\\pi(q) = 90q - q^{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll consider minimizing a function that is concave over its relevant range. In order to maximize a convex function, we can simply take its negatitive (which is concave) and minimize it.\n",
    "\n",
    "For simplicity, we'll only consider function of one real-valued variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a simple method of finding a minimum that can be used on a very wide variety of functions. Gradient Descent along with methods based on it is commonly used to maximize functions that have no second derivative or where the second derivative of the function is difficult to compute or even approximate. Gradient Descent based methods are used in order to minimize the loss function in many machine learning models, particularly deep learning models. \n",
    "\n",
    "The idea is that once we pick an initial guess, we iterativly can follow the slope (i.e. go the opposite direction of the slope) until we reach a minimum. This is often compared to a ball rolling down a hill, but without the details to make it physically realistic. (There are other optimization techniques that draw inspriation for the physical metaphor by adding in analoguaes to friction and gravity.)\n",
    "\n",
    "The iterative equation for Gradient Descent, for a real-valued function of one variable, is given by:\n",
    "$$x_{n+1} = x_{n} - \\gamma \\ f'(x_{n})$$\n",
    "Where $\\gamma$, called the learning rate, controls the speed at which descent happens. The learning rate is an examnple of a hyperparameter. A hyper parameter what is not estimated by the model and much be chosen before the model is run. In cases where the function is very steep, a low value should be chosen for the learning rate. Conversly, A high value for the learning rate should be chosen for very gradual functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Show the expression for profit, $\\pi(q) = 90q - q^{3}$, is correct.\n",
    "2. Mathematically solve for he optimal quantity of widgets Bob should produce to maximize his profit and his maximum profit.  \n",
    "(If you need a refresher, see: https://mjo.osborne.economics.utoronto.ca/index.php/tutorial/index/1/nen/t)\n",
    "3. Use Gradient Descent to numerically solve Bob's problem. Write the algorithm yourself; don't use a package!\n",
    "4. Use Newton-Raphson to numerically solve Bob's problem. Write the algorithm yourself; don't use a package!\n",
    "4. Graphically compare the convergence speed of Gradient Descent and Newton's method.\n",
    "5. Use Scipy's building optimization function to solve Bob's problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (simpy-lab)",
   "language": "python",
   "name": "simpy-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
